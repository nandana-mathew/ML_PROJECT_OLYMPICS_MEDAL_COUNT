{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Olympic Medal Prediction - Data Download and Preparation\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "### 1. Install Required Packages\n",
    "```bash\n",
    "pip install kaggle wbdata pandas numpy requests openpyxl pycountry\n",
    "```\n",
    "\n",
    "### 2. Configure Kaggle API\n",
    "1. Go to https://www.kaggle.com/account\n",
    "2. Click \"Create New API Token\" - downloads `kaggle.json`\n",
    "3. Place it at:\n",
    "   - **Linux/Mac**: `~/.kaggle/kaggle.json`\n",
    "   - **Windows**: `C:\\Users\\<username>\\.kaggle\\kaggle.json`\n",
    "4. Set permissions (Linux/Mac): `chmod 600 ~/.kaggle/kaggle.json`\n",
    "\n",
    "### 3. Run All Cells\n",
    "This notebook will:\n",
    "- Download Olympic medal data from Kaggle\n",
    "- Pull World Bank development indicators\n",
    "- Download UNDP HDI data\n",
    "- Create country ISO3 mappings\n",
    "- Merge everything into `data/processed/olympics_merged.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Directory structure created\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import zipfile\n",
    "import wbdata\n",
    "import pycountry\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create directory structure\n",
    "Path('data/raw/olympic').mkdir(parents=True, exist_ok=True)\n",
    "Path('data/raw/worldbank').mkdir(parents=True, exist_ok=True)\n",
    "Path('data/raw/hdi').mkdir(parents=True, exist_ok=True)\n",
    "Path('data/processed').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"✓ Directory structure created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download Olympic Medal Data from Kaggle\n",
    "\n",
    "We'll use the `olympic-games-medal-tally-by-country` dataset which provides clean country × games medal totals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ramontanoeiro/olympic-games-ceremony-and-medals-data...\n",
      "Dataset URL: https://www.kaggle.com/datasets/ramontanoeiro/olympic-games-ceremony-and-medals-data\n",
      "❌ Error downloading from Kaggle: 403 Client Error: Forbidden for url: https://www.kaggle.com/api/v1/datasets/download/ramontanoeiro/olympic-games-ceremony-and-medals-data?raw=false\n",
      "\n",
      "Troubleshooting:\n",
      "1. Check kaggle.json is in correct location\n",
      "2. Verify you've accepted the dataset terms on Kaggle website\n",
      "3. Try: kaggle datasets download -d ramontanoeiro/olympic-games-ceremony-and-medals-data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def download_kaggle_olympics():\n",
    "    \"\"\"Download Olympic medal dataset using Kaggle API\"\"\"\n",
    "    try:\n",
    "        import kaggle\n",
    "        \n",
    "        # Download the dataset\n",
    "        dataset = 'ramontanoeiro/olympic-games-ceremony-and-medals-data'\n",
    "        print(f\"Downloading {dataset}...\")\n",
    "        \n",
    "        kaggle.api.dataset_download_files(\n",
    "            dataset, \n",
    "            path='data/raw/olympic',\n",
    "            unzip=True\n",
    "        )\n",
    "        \n",
    "        print(\"✓ Olympic data downloaded\")\n",
    "        \n",
    "        # List downloaded files\n",
    "        files = os.listdir('data/raw/olympic')\n",
    "        print(f\"Downloaded files: {files}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error downloading from Kaggle: {e}\")\n",
    "        print(\"\\nTroubleshooting:\")\n",
    "        print(\"1. Check kaggle.json is in correct location\")\n",
    "        print(\"2. Verify you've accepted the dataset terms on Kaggle website\")\n",
    "        print(\"3. Try: kaggle datasets download -d ramontanoeiro/olympic-games-ceremony-and-medals-data\")\n",
    "        return False\n",
    "\n",
    "download_kaggle_olympics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available files: []\n",
      "❌ Could not find medals CSV file\n"
     ]
    }
   ],
   "source": [
    "def load_and_prepare_olympic_data():\n",
    "    \"\"\"Load Olympic data and prepare country-year-medal table\"\"\"\n",
    "    \n",
    "    # Try to load medals file\n",
    "    olympic_files = os.listdir('data/raw/olympic')\n",
    "    print(f\"Available files: {olympic_files}\")\n",
    "    \n",
    "    # Look for medals file\n",
    "    medals_file = None\n",
    "    for f in olympic_files:\n",
    "        if 'medal' in f.lower() and f.endswith('.csv'):\n",
    "            medals_file = f\n",
    "            break\n",
    "    \n",
    "    if medals_file:\n",
    "        df = pd.read_csv(f'data/raw/olympic/{medals_file}')\n",
    "        print(f\"✓ Loaded {medals_file}\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(f\"Columns: {df.columns.tolist()}\")\n",
    "        print(f\"\\nFirst few rows:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # Aggregate by country and year\n",
    "        if 'country_name' in df.columns and 'slug_game' in df.columns:\n",
    "            # Extract year and season from slug_game\n",
    "            df['year'] = df['slug_game'].str.extract(r'(\\d{4})')[0].astype(int)\n",
    "            df['games_type'] = df['slug_game'].apply(\n",
    "                lambda x: 'Winter' if 'winter' in x.lower() else 'Summer'\n",
    "            )\n",
    "            \n",
    "            # Aggregate medals\n",
    "            agg_dict = {\n",
    "                'medal_type': 'count',  # Will be renamed to total_medals\n",
    "            }\n",
    "            \n",
    "            # Count medal types\n",
    "            df['gold'] = (df['medal_type'] == 'GOLD').astype(int)\n",
    "            df['silver'] = (df['medal_type'] == 'SILVER').astype(int)\n",
    "            df['bronze'] = (df['medal_type'] == 'BRONZE').astype(int)\n",
    "            \n",
    "            olympic_df = df.groupby(['country_name', 'year', 'games_type']).agg({\n",
    "                'gold': 'sum',\n",
    "                'silver': 'sum',\n",
    "                'bronze': 'sum',\n",
    "                'medal_type': 'count'\n",
    "            }).reset_index()\n",
    "            \n",
    "            olympic_df.rename(columns={'medal_type': 'total_medals'}, inplace=True)\n",
    "            \n",
    "        else:\n",
    "            olympic_df = df\n",
    "        \n",
    "        # Save processed Olympic data\n",
    "        olympic_df.to_csv('data/raw/olympic/medals_processed.csv', index=False)\n",
    "        print(f\"\\n✓ Processed Olympic data saved: {olympic_df.shape}\")\n",
    "        \n",
    "        return olympic_df\n",
    "    \n",
    "    else:\n",
    "        print(\"❌ Could not find medals CSV file\")\n",
    "        return None\n",
    "\n",
    "olympic_df = load_and_prepare_olympic_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Country ISO3 Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_country_mapping():\n",
    "    \"\"\"Create mapping from country names to ISO3 codes\"\"\"\n",
    "    \n",
    "    # Common Olympic country name variations\n",
    "    manual_mappings = {\n",
    "        'United States': 'USA',\n",
    "        'United States of America': 'USA',\n",
    "        'Great Britain': 'GBR',\n",
    "        'Russia': 'RUS',\n",
    "        'Russian Federation': 'RUS',\n",
    "        'ROC': 'RUS',  # Russian Olympic Committee\n",
    "        'Soviet Union': 'RUS',\n",
    "        'China': 'CHN',\n",
    "        \"People's Republic of China\": 'CHN',\n",
    "        'South Korea': 'KOR',\n",
    "        'Korea': 'KOR',\n",
    "        'Chinese Taipei': 'TWN',\n",
    "        'Hong Kong': 'HKG',\n",
    "        'Iran': 'IRN',\n",
    "        'Netherlands': 'NLD',\n",
    "        'Czech Republic': 'CZE',\n",
    "        'Czechia': 'CZE',\n",
    "        'North Korea': 'PRK',\n",
    "        'Vietnam': 'VNM',\n",
    "        'Venezuela': 'VEN',\n",
    "        'Syria': 'SYR',\n",
    "        'Tanzania': 'TZA',\n",
    "        'Bahamas': 'BHS',\n",
    "        'Philippines': 'PHL',\n",
    "        'Moldova': 'MDA',\n",
    "        'Ivory Coast': 'CIV',\n",
    "        \"Côte d'Ivoire\": 'CIV',\n",
    "    }\n",
    "    \n",
    "    def get_iso3(country_name):\n",
    "        \"\"\"Get ISO3 code for a country name\"\"\"\n",
    "        # Check manual mappings first\n",
    "        if country_name in manual_mappings:\n",
    "            return manual_mappings[country_name]\n",
    "        \n",
    "        # Try pycountry fuzzy search\n",
    "        try:\n",
    "            result = pycountry.countries.search_fuzzy(country_name)\n",
    "            return result[0].alpha_3\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    # Create mapping from Olympic data\n",
    "    if olympic_df is not None and 'country_name' in olympic_df.columns:\n",
    "        unique_countries = olympic_df['country_name'].unique()\n",
    "        \n",
    "        mapping_data = []\n",
    "        for country in unique_countries:\n",
    "            iso3 = get_iso3(country)\n",
    "            mapping_data.append({\n",
    "                'country_name': country,\n",
    "                'iso3': iso3\n",
    "            })\n",
    "        \n",
    "        mapping_df = pd.DataFrame(mapping_data)\n",
    "        \n",
    "        # Report unmapped countries\n",
    "        unmapped = mapping_df[mapping_df['iso3'].isnull()]\n",
    "        if len(unmapped) > 0:\n",
    "            print(f\"⚠️  Unmapped countries ({len(unmapped)}):\")\n",
    "            print(unmapped['country_name'].tolist())\n",
    "        \n",
    "        # Save mapping\n",
    "        mapping_df.to_csv('data/raw/country_iso3_mapping.csv', index=False)\n",
    "        print(f\"\\n✓ Country mapping created: {len(mapping_df)} countries\")\n",
    "        print(f\"  Mapped: {len(mapping_df[mapping_df['iso3'].notnull()])}\")\n",
    "        \n",
    "        return mapping_df\n",
    "    \n",
    "    return None\n",
    "\n",
    "country_mapping = create_country_mapping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Download World Bank Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading World Bank data from 1960 to 2024...\n",
      "  Downloading population (SP.POP.TOTL)... ❌ Error: got an unexpected keyword argument 'convert_date'\n",
      "  Downloading gdp_current_usd (NY.GDP.MKTP.CD)... ❌ Error: got an unexpected keyword argument 'convert_date'\n",
      "  Downloading gdp_per_capita (NY.GDP.PCAP.CD)... ❌ Error: got an unexpected keyword argument 'convert_date'\n",
      "  Downloading life_expectancy (SP.DYN.LE00.IN)... ❌ Error: got an unexpected keyword argument 'convert_date'\n",
      "  Downloading adult_literacy_rate (SE.ADT.LITR.ZS)... ❌ Error: got an unexpected keyword argument 'convert_date'\n",
      "  Downloading unemployment_rate (SL.UEM.TOTL.ZS)... ❌ Error: got an unexpected keyword argument 'convert_date'\n"
     ]
    }
   ],
   "source": [
    "def download_world_bank_data(start_year=1960, end_year=2024):\n",
    "    \"\"\"Download World Bank indicators using wbdata\"\"\"\n",
    "    \n",
    "    indicators = {\n",
    "        'SP.POP.TOTL': 'population',\n",
    "        'NY.GDP.MKTP.CD': 'gdp_current_usd',\n",
    "        'NY.GDP.PCAP.CD': 'gdp_per_capita',\n",
    "        'SP.DYN.LE00.IN': 'life_expectancy',\n",
    "        'SE.ADT.LITR.ZS': 'adult_literacy_rate',\n",
    "        'SL.UEM.TOTL.ZS': 'unemployment_rate'\n",
    "    }\n",
    "    \n",
    "    print(f\"Downloading World Bank data from {start_year} to {end_year}...\")\n",
    "    \n",
    "    date_range = (datetime(start_year, 1, 1), datetime(end_year, 12, 31))\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for wb_code, var_name in indicators.items():\n",
    "        print(f\"  Downloading {var_name} ({wb_code})...\", end=' ')\n",
    "        \n",
    "        try:\n",
    "            data = wbdata.get_dataframe(\n",
    "                {wb_code: var_name},\n",
    "                date=date_range,\n",
    "                convert_date=True\n",
    "            )\n",
    "            \n",
    "            # Reset index to get country and date as columns\n",
    "            data = data.reset_index()\n",
    "            data['year'] = data['date'].dt.year\n",
    "            data = data[['country', 'year', var_name]]\n",
    "            \n",
    "            # Save individual indicator\n",
    "            data.to_csv(f'data/raw/worldbank/{var_name}.csv', index=False)\n",
    "            \n",
    "            all_data.append(data)\n",
    "            print(f\"✓ ({len(data)} rows)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {e}\")\n",
    "    \n",
    "    # Merge all indicators\n",
    "    if all_data:\n",
    "        wb_combined = all_data[0]\n",
    "        for df in all_data[1:]:\n",
    "            wb_combined = wb_combined.merge(df, on=['country', 'year'], how='outer')\n",
    "        \n",
    "        # Add ISO3 codes\n",
    "        def get_iso3_wb(country_name):\n",
    "            try:\n",
    "                result = pycountry.countries.search_fuzzy(country_name)\n",
    "                return result[0].alpha_3\n",
    "            except:\n",
    "                return None\n",
    "        \n",
    "        wb_combined['iso3'] = wb_combined['country'].apply(get_iso3_wb)\n",
    "        \n",
    "        # Save combined data\n",
    "        wb_combined.to_csv('data/raw/worldbank_combined.csv', index=False)\n",
    "        print(f\"\\n✓ World Bank combined data saved: {wb_combined.shape}\")\n",
    "        print(f\"  Countries with ISO3: {wb_combined['iso3'].notnull().sum() / len(wb_combined) * 100:.1f}%\")\n",
    "        \n",
    "        return wb_combined\n",
    "    \n",
    "    return None\n",
    "\n",
    "wb_data = download_world_bank_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Download UNDP HDI Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading HDI data...\n",
      "✓ HDI data downloaded: (206, 1008)\n",
      "Columns: ['iso3', 'country', 'hdicode', 'region', 'hdi_rank_2021', 'hdi_1990', 'hdi_1991', 'hdi_1992', 'hdi_1993', 'hdi_1994']...\n"
     ]
    }
   ],
   "source": [
    "def download_hdi_data():\n",
    "    \"\"\"Download UNDP Human Development Index data\"\"\"\n",
    "    \n",
    "    print(\"Downloading HDI data...\")\n",
    "    \n",
    "    # UNDP HDI data URL (latest available)\n",
    "    url = 'https://hdr.undp.org/sites/default/files/2021-22_HDR/HDR21-22_Composite_indices_complete_time_series.csv'\n",
    "    \n",
    "    try:\n",
    "        hdi_df = pd.read_csv(url)\n",
    "        \n",
    "        print(f\"✓ HDI data downloaded: {hdi_df.shape}\")\n",
    "        print(f\"Columns: {hdi_df.columns.tolist()[:10]}...\")  # Show first 10 columns\n",
    "        \n",
    "        # Reshape from wide to long format\n",
    "        # Find year columns (typically 1990-2021)\n",
    "        year_cols = [col for col in hdi_df.columns if col.isdigit()]\n",
    "        \n",
    "        if year_cols:\n",
    "            id_cols = [col for col in hdi_df.columns if col not in year_cols]\n",
    "            hdi_long = hdi_df.melt(\n",
    "                id_vars=id_cols,\n",
    "                value_vars=year_cols,\n",
    "                var_name='year',\n",
    "                value_name='hdi'\n",
    "            )\n",
    "            hdi_long['year'] = hdi_long['year'].astype(int)\n",
    "            \n",
    "            # Keep relevant columns\n",
    "            keep_cols = ['country', 'iso3', 'year', 'hdi']\n",
    "            available_cols = [c for c in keep_cols if c in hdi_long.columns]\n",
    "            \n",
    "            if 'iso3' not in available_cols and 'country' in available_cols:\n",
    "                # Add ISO3 codes\n",
    "                def get_iso3_hdi(country_name):\n",
    "                    try:\n",
    "                        result = pycountry.countries.search_fuzzy(str(country_name))\n",
    "                        return result[0].alpha_3\n",
    "                    except:\n",
    "                        return None\n",
    "                hdi_long['iso3'] = hdi_long['country'].apply(get_iso3_hdi)\n",
    "                available_cols.append('iso3')\n",
    "            \n",
    "            hdi_clean = hdi_long[available_cols].copy()\n",
    "            \n",
    "            # Remove missing HDI values\n",
    "            hdi_clean = hdi_clean[hdi_clean['hdi'].notnull()]\n",
    "            \n",
    "            # Save\n",
    "            hdi_clean.to_csv('data/raw/hdi/hdi_data.csv', index=False)\n",
    "            print(f\"✓ HDI data processed and saved: {hdi_clean.shape}\")\n",
    "            \n",
    "            return hdi_clean\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error downloading HDI data: {e}\")\n",
    "        print(\"Continuing without HDI data...\")\n",
    "        return None\n",
    "\n",
    "hdi_data = download_hdi_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Merge All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MERGING ALL DATASETS\n",
      "============================================================\n",
      "❌ No Olympic data available\n"
     ]
    }
   ],
   "source": [
    "def merge_all_data():\n",
    "    \"\"\"Merge Olympic, World Bank, and HDI data\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MERGING ALL DATASETS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Start with Olympic data\n",
    "    if olympic_df is None:\n",
    "        print(\"❌ No Olympic data available\")\n",
    "        return None\n",
    "    \n",
    "    merged = olympic_df.copy()\n",
    "    print(f\"Starting with Olympic data: {merged.shape}\")\n",
    "    \n",
    "    # Add ISO3 codes\n",
    "    if country_mapping is not None:\n",
    "        merged = merged.merge(\n",
    "            country_mapping,\n",
    "            on='country_name',\n",
    "            how='left'\n",
    "        )\n",
    "        print(f\"After adding ISO3: {merged.shape}\")\n",
    "        print(f\"  Missing ISO3: {merged['iso3'].isnull().sum()}\")\n",
    "    \n",
    "    # Merge World Bank data\n",
    "    if wb_data is not None:\n",
    "        wb_subset = wb_data[wb_data['iso3'].notnull()].copy()\n",
    "        \n",
    "        merged = merged.merge(\n",
    "            wb_subset,\n",
    "            on=['iso3', 'year'],\n",
    "            how='left',\n",
    "            suffixes=('', '_wb')\n",
    "        )\n",
    "        print(f\"After merging World Bank: {merged.shape}\")\n",
    "        \n",
    "        # For missing years, use nearest previous year (forward fill by country)\n",
    "        wb_indicators = ['population', 'gdp_current_usd', 'gdp_per_capita', \n",
    "                        'life_expectancy', 'adult_literacy_rate', 'unemployment_rate']\n",
    "        \n",
    "        for indicator in wb_indicators:\n",
    "            if indicator in merged.columns:\n",
    "                merged[indicator] = merged.groupby('iso3')[indicator].fillna(method='ffill')\n",
    "    \n",
    "    # Merge HDI data\n",
    "    if hdi_data is not None:\n",
    "        hdi_subset = hdi_data[hdi_data['iso3'].notnull()][['iso3', 'year', 'hdi']].copy()\n",
    "        \n",
    "        merged = merged.merge(\n",
    "            hdi_subset,\n",
    "            on=['iso3', 'year'],\n",
    "            how='left'\n",
    "        )\n",
    "        print(f\"After merging HDI: {merged.shape}\")\n",
    "        \n",
    "        # Forward fill HDI by country\n",
    "        merged['hdi'] = merged.groupby('iso3')['hdi'].fillna(method='ffill')\n",
    "    \n",
    "    # Reorder columns\n",
    "    base_cols = ['iso3', 'country_name', 'year', 'games_type']\n",
    "    medal_cols = ['gold', 'silver', 'bronze', 'total_medals']\n",
    "    \n",
    "    other_cols = [c for c in merged.columns if c not in base_cols + medal_cols]\n",
    "    \n",
    "    merged = merged[base_cols + medal_cols + other_cols]\n",
    "    \n",
    "    # Remove rows without ISO3 (can't be matched)\n",
    "    merged = merged[merged['iso3'].notnull()]\n",
    "    \n",
    "    # Save final merged data\n",
    "    merged.to_csv('data/processed/olympics_merged.csv', index=False)\n",
    "    \n",
    "    print(f\"\\n✓ FINAL MERGED DATA: {merged.shape}\")\n",
    "    print(f\"\\nColumns: {merged.columns.tolist()}\")\n",
    "    print(f\"\\nData coverage:\")\n",
    "    print(f\"  Years: {merged['year'].min()} - {merged['year'].max()}\")\n",
    "    print(f\"  Countries: {merged['iso3'].nunique()}\")\n",
    "    print(f\"  Summer Games: {(merged['games_type']=='Summer').sum()}\")\n",
    "    print(f\"  Winter Games: {(merged['games_type']=='Winter').sum()}\")\n",
    "    print(f\"\\nMissing data:\")\n",
    "    print(merged.isnull().sum())\n",
    "    \n",
    "    print(f\"\\n✓ Saved to: data/processed/olympics_merged.csv\")\n",
    "    \n",
    "    return merged\n",
    "\n",
    "final_data = merge_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample of final data\n",
    "if final_data is not None:\n",
    "    print(\"\\nSample of merged data:\")\n",
    "    print(final_data.head(10))\n",
    "    \n",
    "    print(\"\\nSummary statistics:\")\n",
    "    print(final_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Files Created:\n",
    "- `data/raw/olympic/` - Raw Olympic medal data\n",
    "- `data/raw/worldbank/` - Individual World Bank indicator files\n",
    "- `data/raw/worldbank_combined.csv` - Combined World Bank data\n",
    "- `data/raw/hdi/hdi_data.csv` - UNDP HDI data\n",
    "- `data/raw/country_iso3_mapping.csv` - Country name to ISO3 mapping\n",
    "- `data/processed/olympics_merged.csv` - **Final merged dataset**\n",
    "\n",
    "### Next Steps:\n",
    "1. Run `train_two_stage_model.ipynb` to train the prediction model\n",
    "2. Use `evaluate_and_report.ipynb` to evaluate and generate reports\n",
    "3. Launch `streamlit run app.py` for interactive predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
